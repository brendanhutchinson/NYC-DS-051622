{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Ensemble Learning: Gradient Boosting\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC July 2022\n",
    "<p>Phase 3: Topic 31</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Boosting\n",
    "- Besides bagging: other major framework for ensemble tree learning\n",
    "- Different philosophies of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Ralph Nader philosophy of learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images\\mistake_nader.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images\\nader.jpeg\" />\n",
    "<center> Nader giving you life lessons about learning theory AND the true source of income inequality in this country. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Boosting is essentially this:\n",
    "- at each step learner trains on mistakes of previous step.\n",
    "- uses knowledge of mistakes to correct predictions in next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**More technically**\n",
    "- Trying to approximate collection of targets $\\{y_i\\}$ with a function $F(x_i)$.\n",
    "\n",
    "Boosting: sequentially update F via step-by-step learning from errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/boosting_update.png\" width = 500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But there's a general problem with this:\n",
    "- what if what we learn from last mistake is gleaned from a set of situations that are too specific?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**More technically**\n",
    "\n",
    "Weighting last mistake with respect to the specific training data too strongly.\n",
    "\n",
    "- Our corrections to how we predict will then be too specific to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**I just overfitted**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes I learned from a specific set of mistakes:\n",
    "- but have I learned a sufficiently general lesson?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Want to learn from last mistakes in a way that is **generalizable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One possibility: \n",
    "- don't weight lesson from any one mistake too strongly.\n",
    "- perhaps even learn from different mistakes in a \"weak\" manner\n",
    "- BUT do this a bunch of times in sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The hope is that eventually:\n",
    "\n",
    "- Gain wisdom via each generation learning from the previous \n",
    "- But in a weak way: take some of the \"lessons\" but not all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><b> Can such a sequence of weak learners create a single strong learner? </b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The answer is yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/boosting_update.png\" width = 500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Getting better approximations on $F$ iteratively.\n",
    "- Looks a lot like gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use regression as example:\n",
    "\n",
    "- Least squares objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "from sympy.abc import x, y\n",
    "\n",
    "x, y, i, N, h, F = symbols(\"x, y, i, N, h_0, F\")\n",
    "L = summation((Indexed('y',i) - F)**2 ,(i,1,N))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\sum_{i=1}^{N} \\left(- F + {y}_{i}\\right)^{2}}{2}$"
      ],
      "text/plain": [
       "Sum((-F + y[i])**2, (i, 1, N))/2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Take gradient with respect to the function $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gradL = diff(L, F)\n",
    "gradL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\nabla_FL = \\sum_{i=1}^N \\Big(F(x_i)- y_i\\Big) $$\n",
    "\n",
    "or $$ - \\nabla_FL = \\sum_{i=1}^N  \\Big(y_i - F(x_i)\\Big) $$\n",
    "**This is  error we input to learner at each step**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Input into a regressor:\n",
    "\n",
    "Want to learn from errors at given stage $m$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/train_mistakes.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have a statistical model that can predict error from previous step:\n",
    "\n",
    "- Learned function:\n",
    "$$h_m(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/error_prediction.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Adding this to previous estimator: previous estimator + training error\n",
    "    \n",
    "$$ F_{m+1}(x_i) = F_m(x_i) + h_m(x_i) $$\n",
    "\n",
    "Correct for error in previous stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is good, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not necessarily. If regressor is good at fitting error:\n",
    "\n",
    "- this is a super-strong learner:\n",
    "- Accounts too strongly for specific training errors  at step $m$.\n",
    "\n",
    "**Will introduce variance problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A way to make learner weak: \n",
    "- simpler decision trees\n",
    "- the learning rate $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Weaker vs. stronger learners: decision tree depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><b>Strong Learner </b> </center>\n",
    "<img src = \"Images/deep_tree.png\" />\n",
    "<center> Will train on errors at each step very well. But probably too well. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><b>Weak learner </b></center>\n",
    "<center><img src = \"Images/dec_stump.png\" width = 250/></center>\n",
    "\n",
    "<center> Decision boundary learned by stump: </center>\n",
    "<center><img src = \"Images/dectree_stump_boundary.png\" width = 250/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead of:\n",
    "    \n",
    "$$ F_{m+1}(x_i) = F_m(x_i) + h_m(x_i) $$\n",
    "\n",
    "with $ h_m(x_i)$ learned by tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Weight learning from mth mistake weakly:\n",
    "\n",
    "$$ F_{m+1}(x_i) = F_m(x_i) + \\alpha h_m(x_i) $$\n",
    "\n",
    "where $\\alpha$ is small. \n",
    "\n",
    "- Weakens effect of learning from error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The learning rate $\\alpha$ controls how much we weight learners:\n",
    "\n",
    "- We weight how we factor in our learning from mistakes of a given step weakly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The learning rate (one factor controlling weakness of learners): connects to gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ F_{m+1}(x_i) = F_m(x_i) + \\alpha h_m(x_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ F_{m+1}(x_i) = F_m(x_i) + \\alpha \\Big(y_i - F_m(x_i)\\Big)_{estimated} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But we know that:\n",
    "$$ - \\nabla_F L|_{x_i} = \\sum_i^{N} \\Big(y_i - F_m(x_i)\\Big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ F_{m+1}(x_i) = F_m(x_i) + \\alpha \\Big(y_i - F_m(x_i)\\Big)_{estimated} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ F_{m+1}(x_i) = F_m(x_i) - \\alpha \\nabla_F L|_{x_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ F_{m+1}(x_i) = F_m(x_i) - \\alpha \\nabla_F L|_{x_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Doing gradient descent:\n",
    "- iteratively adding onto (or boosting) estimator to lower loss.\n",
    "- Parameter $\\alpha$ controlling gradient step also representing weakness of learning at each stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's why its called gradient boosting!\n",
    "\n",
    "General framework: with different $L$ applies to classification, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A summary of the learning process in more detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/update_detailed_process.png\" width = 800/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Fit a noisy sinusoid with boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.linspace(0,40,1000)\n",
    "y = np.sin(X) + norm.rvs(loc = 0, scale = .4, size = 1000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Starting point: the dummy model\n",
    "\n",
    "Dummy regressor: average of our $\\{y_i\\}$ as a model of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture dummyapprox\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y, label = 'Data')\n",
    "f0 = y.mean()\n",
    "ax.hlines(f0, 0, 40, linestyle = '--', color = 'r', label = 'Dummy Regressor')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('X')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dummyapprox()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now walk through process of \"boosting\" this prediction with weak learner sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def simple_boosting_algorithm(X, y, n_learners, learner,\n",
    "                              learning_rate):\n",
    "    y = y.ravel()\n",
    "    # calculates the dummy model\n",
    "    f0 = y.mean()\n",
    "    \n",
    "    # calculates error of first step\n",
    "    residuals = y - f0\n",
    "    \n",
    "    # This next line fills an array of len(y) with the mean of y.\n",
    "    f = np.full(len(y), fill_value=f0)\n",
    "\n",
    "    # start sequential training \n",
    "\n",
    "    for i in range(n_learners):\n",
    "        # error of previous model\n",
    "        residuals = y - f\n",
    "        \n",
    "        # fit error with decision tree\n",
    "        mod = learner.fit(X.reshape(-1, 1), residuals)\n",
    "\n",
    "        # update f\n",
    "        f = learning_rate * mod.predict(X.reshape(-1, 1)) + f\n",
    "        fit_df = pd.DataFrame({'x': X, 'F': f})\n",
    "    return fit_df\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from bokeh.layouts import column,row\n",
    "from bokeh.models import ColumnDataSource, Slider, TextInput, Select\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.themes import Theme\n",
    "import yaml\n",
    "import numpy as np\n",
    "output_notebook()\n",
    "\n",
    "def bk_app(doc):\n",
    "    \n",
    "    func_approx =simple_boosting_algorithm(X=X,\n",
    "                      y=y,\n",
    "                      n_learners= 1,\n",
    "                      learner=DecisionTreeRegressor(max_depth= 1),\n",
    "                      learning_rate=0.02)\n",
    "\n",
    "    source = ColumnDataSource(func_approx)\n",
    "\n",
    "    # Create plots and widgets\n",
    "    plot = figure()\n",
    "\n",
    "    plot.circle(X, y, size = 6, color = 'blue', legend_label= 'Data')\n",
    "    plot.line('x', 'F', source = source, line_width=3, line_alpha=1, color = 'red', legend_label = 'Boosting') \n",
    "\n",
    "    # Create Slider object\n",
    "     \n",
    "    tree_depth = Slider(start=1, end=10, value=1,\n",
    "                    step=1, title='Tree Depth')  \n",
    "    \n",
    "    n_est = TextInput(title=\"Number of estimators\", value = '1')\n",
    "    \n",
    "    lr = Select(title=\"Learning rate\", value='5e-1',\n",
    "               options=['1e-4', '1e-3', '5e-3', '1e-2', '5e-2', '1e-1', '5e-1', '1'])\n",
    "\n",
    "    # Adding callback code\n",
    "    def callback(attr, old, new):\n",
    "        N = n_est.value\n",
    "        depth = tree_depth.value\n",
    "        learn_rate = float(lr.value)\n",
    "        \n",
    "        func_approx =simple_boosting_algorithm(X=X,\n",
    "                              y=y,\n",
    "                              n_learners= int(N),\n",
    "                              learner=DecisionTreeRegressor(max_depth= depth),\n",
    "                              learning_rate= float(learn_rate))\n",
    "\n",
    "        source.data = func_approx\n",
    "\n",
    "\n",
    "    tree_depth.on_change('value', callback)\n",
    "    n_est.on_change('value', callback)\n",
    "    lr.on_change('value', callback)\n",
    "\n",
    "    doc.add_root(row(\n",
    "        plot,\n",
    "        column(tree_depth, n_est, lr),\n",
    "    ))\n",
    "\n",
    "    doc.theme = Theme(json=yaml.load(\"\"\"\n",
    "        attrs:\n",
    "            Figure:\n",
    "                background_fill_color: white\n",
    "                outline_line_color: white\n",
    "                toolbar_location: above\n",
    "                height: 450\n",
    "                width: 450\n",
    "    \"\"\", Loader=yaml.FullLoader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The effects of sequential boosting with weak learners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "show(bk_app, notebook_url=\"http://localhost:8888\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gradient boosting:\n",
    "    \n",
    "- Tuning hyperparameters can be very important for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Number of estimators (convergence/overfitting)\n",
    "- learning rate (too high...overfit)\n",
    "- tree depth (too large...overfit)\n",
    "\n",
    "Critical hyperparameters in balance between underfitting/overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Playing around with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "show(bk_app, notebook_url=\"http://localhost:8888\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why do all this?\n",
    "\n",
    "- Decision trees can be very fast.\n",
    "- Weak learners (particularly stumps) are even **faster**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In principal:\n",
    "- can tune and fit models really quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our naive implementation: \n",
    "- can be a little sensitive to overfitting \n",
    "- certainly overfits more than random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Gets much better with extra randomization: \n",
    "- Random feature subset selection on individual trees \n",
    "- Randomly sampling subset of training data to improve on error at each stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Even better with regularization on trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = \\sum_{i=1}^N l(y_i, \\hat{y}_i) + \\gamma \\sum_{m=1}^M \\Omega(h_m)$$\n",
    "- $\\Omega$: complexity of tree at the mth step. \n",
    "- $\\gamma$: controls penalty on complexity \n",
    "\n",
    "**Penalizing building more complex trees but also allows for it if necessary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\gamma$ controls tree pruning. If branch doesn't minimize error (maximize imformation gain) **enough** then cut it.\n",
    "\n",
    "<img src = \"Images/pruning_reg.webp\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Use above strategies + a lot of algorithm optimization\n",
    "\n",
    "- XGBoost (Extreme Gradient Boosting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/xgboost.png\" />\n",
    "<center>Package integrates well with scikit-learn</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Load in the XGB classifier/regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Hyperparameters to tune and ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| Hyperparameter | Description  | Typical ranges | <center> Comments </center> |\n",
    "| --- | --- | --- | --- |\n",
    "| n_estimator | Number of trees <br> (iterations in sequence) | 50-500 | <center> Can get into low thousands. <br> Increasing beyond certain point: <br> overfitting or no benefit. </center>  |\n",
    "| max_depth | Maximum tree depth of learners | 3-6 |<center> Increment by 1. <br> Changing depth: huge effect. </center>| \n",
    "| learning_rate | The learning rate | 1e-3 to 1 | <center> Proper regularization/randomization <br> allows for faster learning rates <br> ~ 0.1-1 </center>   |\n",
    "|  <font color='red'>gamma </font>| Tree complexity regularization | 0 - 100 | <center> Primary knob for tree regularization </center>   |\n",
    "| <font color='red'>colsample_bytree </font> | <center> Fraction of features <br> randomly sampled by tree </center> | 0.5 - 1 | <center> Regularizing effect  <br>colsample_bylevel, colsample_bynode:<br> add extra degrees of randomization </center>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "XGB estimators fits neatly and seamlessly into scikit-learn model pipelines, grid search, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "params = {'max_depth': [3,4, 5], 'learning_rate': [.1, .3, .5],\n",
    "          'gamma': [0,5,10], 'n_estimators': [50,100, 150] }\n",
    "cv = GridSearchCV(estimator = XGBRegressor(objective='reg:squarederror'), scoring='neg_mean_absolute_error',\n",
    "                  param_grid = params, cv = 5)\n",
    "cv.fit(X_train.reshape(-1,1), y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "best_mod = cv.best_estimator_\n",
    "best_mod.fit(X_train.reshape(-1,1), y_train);\n",
    "y_pred = best_mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "total_preds = best_mod.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_test, y_test, c ='g', label = 'Test data')\n",
    "ax.scatter(X_test, y_pred, c ='r', label = 'Prediction')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clearly doing pretty well with variance\n",
    "- CV average MAE at scale of intrinsic noise in model.\n",
    "- Also doing decently with bias. Sine wave amplitude ~ 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How about something a little more complicated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "file_path = \"Data/WHO_life.csv\"\n",
    "who_df = pd.read_csv(file_path).drop(columns = ['Adult Mortality', 'infant deaths',\n",
    "                                            'Year', 'Status', ' thinness 5-9 years', 'Country', \n",
    "                                            'under-five deaths ']).dropna()\n",
    "# clean column names\n",
    "who_df.columns = who_df.columns.str.strip()\n",
    "\n",
    "X_who, y_who = who_df.drop(columns = ['Life expectancy']), who_df['Life expectancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_who.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_who.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_who_train, X_who_test, y_who_train, y_who_test = train_test_split(X_who, y_who, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "params = {'max_depth': [3,4, 5], 'learning_rate': [.1, .3, .5],\n",
    "          'gamma': [0,5,10], 'colsample_bynode': [.5, .75, 1], 'n_estimators': [50,100, 150] }\n",
    "cv = GridSearchCV(estimator = XGBRegressor(objective='reg:squarederror'), scoring='neg_mean_absolute_error',\n",
    "                  param_grid = params, cv = 5)\n",
    "cv.fit(X_who_train, y_who_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "best_regressor = cv.best_estimator_\n",
    "best_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "best_regressor.fit(X_who_train, y_who_train)\n",
    "y_pred_train = best_regressor.predict(\n",
    "    X_who_train)\n",
    "y_pred_test = best_regressor.predict(X_who_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_who_test, \n",
    "                          y_pred_test)\n",
    "print(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(y_who_test,\n",
    "                                  y_pred_test))\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Train and test: $R^2$ score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r2_score(y_who_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r2_score(y_who_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparing this to our simple multiple linear regression:\n",
    "- $R^2 = 0.76$ \n",
    "- MAE of ~3.5.\n",
    "- Had to carefully remove correlated features.\n",
    "- Standardize for feature weight importance or regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(best_regressor.feature_importances_,\n",
    "             index = X_who.columns).sort_values(ascending = False)\n",
    "fig, ax = plt.subplots()\n",
    "feat_imp.plot(kind = 'barh', ax = ax)\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('XGB Regressor: Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exact same with classification:\n",
    "- XGBClassifier()\n",
    "- basically same hyperparameters\n",
    "- just different objective function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Gradient boosting\n",
    "\n",
    "- Needs tuning\n",
    "- But extremely fast and effective (as has been seen)\n",
    "- Along with random forest: workhorse of classification/regression in many professional workflows."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
